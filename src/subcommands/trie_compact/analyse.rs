use std::{sync::{Arc, mpsc, Mutex}, time::Instant, collections::{VecDeque, BTreeSet}};

use casper_execution_engine::{
    core::engine_state::{
        purge::{PurgeConfig, PurgeResult},
        EngineConfig, EngineState,
    },
    shared::newtypes::CorrelationId,
    storage::{global_state::{lmdb::LmdbGlobalState, StateProvider}, trie_store::lmdb::LmdbTrieStore},
};
use casper_hashing::Digest;
use casper_types::{EraId, Key};
use clap::{Arg, ArgMatches, Command};
use indicatif::ProgressBar;

use crate::common::execution_engine::create_lmdb_environment;

pub const COMMAND_NAME: &str = "analyse";
const DB_PATH: &str = "file-path";
const _MAX_DB_SIZE: &str = "max-db-size";
const STATE_ROOT_HASH: &str = "state-root-hash";
const DEFAULT_MAX_DB_SIZE: usize = 483_183_820_800; // 450 gb

pub fn command(_display_order: usize) -> Command<'static> {
    Command::new(COMMAND_NAME)
        .about(
            "Reduces the disk size of an LMDB database generated by a Casper node by removing \
            empty blocks of the sparse file.",
        )
        .arg(
            Arg::new(DB_PATH)
                .value_name("DB_PATH")
                .required(true)
                .help("Path to the storage.lmdb or data.lmdb file."),
        )
        .arg(
            Arg::new(STATE_ROOT_HASH)
                .value_name("STATE_ROOT_HASH")
                .required(true)
                .help("State root hash of the tip of the trie."),
        )
}

#[derive(Default)]
struct Stats {
    occupied_max: Option<usize>,
    affix_max: Option<usize>,
}

pub fn run(matches: &ArgMatches) -> bool {
    let db_path = matches.value_of(DB_PATH).unwrap();
    let state_root_hash = matches.value_of(STATE_ROOT_HASH).unwrap();
    let state_root_hash = Digest::from_hex(state_root_hash).unwrap();
    let lmdb_environment = create_lmdb_environment(&db_path, DEFAULT_MAX_DB_SIZE, 15, true)
        .expect("create lmdb environment");

    let lmdb_trie_store = Arc::new(LmdbTrieStore::open(&lmdb_environment, None).unwrap());

    let (empty_root_hash, _empty_trie) =
        casper_execution_engine::storage::global_state::lmdb::compute_empty_root_hash().unwrap();
    let global_state = LmdbGlobalState::new(
        Arc::clone(&lmdb_environment),
        lmdb_trie_store,
        empty_root_hash,
    );

    let pool = rayon::ThreadPoolBuilder::new().num_threads(10).build().unwrap();

    // let mut queue = VecDeque::from_iter([state_root_hash]);

    let mut leafs = 0usize;
    // let mut nodes = 0usize;

    // let mut node_occupied = Vec::new();
    // let mut extensions = Vec::new();

    let pb = ProgressBar::new(1);
    let mut visited = BTreeSet::new();
    // pb.set_length(queue.len());

    let (tx, rx) = mpsc::channel();
    // let tx = Arc::new(tx);
    tx.send(state_root_hash).unwrap();

    let mut stats = Arc::new(Mutex::new(Stats::default()));


    while let Ok(digest) = rx.recv() {
        assert!(visited.insert(digest));
        let global_state = global_state.clone();
        let pb = pb.clone();
        // let tx = Arc::clone(&tx);
        let tx = tx.clone();
        let stats = Arc::clone(&stats);

        pool.spawn(move || {
            let trie = global_state.get_trie(CorrelationId::new(), &digest).expect("should get trie").expect("should have trie");
            let children: Vec<Digest> = trie.iter_children().collect();

            match trie {
                casper_execution_engine::storage::trie::Trie::Leaf { key, value } => {
                    leafs += 1;
                },
                casper_execution_engine::storage::trie::Trie::Node { pointer_block } => {
                    let indexed_pointers: Vec<Digest> = pointer_block.as_indexed_pointers().map(|(_idx, digest)| *digest.hash()).collect();

                    let mut stats = stats.lock().unwrap();
                    match &mut stats.occupied_max {
                        Some(value) => {
                            if indexed_pointers.len() > *value {
                                *value = indexed_pointers.len();
                                pb.println(&format!("New max node occupied list {}", *value));
                            }
                        }
                        None => {
                            stats.occupied_max = Some(indexed_pointers.len());
                        }
                    }
                },
                casper_execution_engine::storage::trie::Trie::Extension { affix, pointer } => {

                    let mut stats = stats.lock().unwrap();
                    match &mut stats.affix_max {
                        Some(value) => {
                            if affix.len() > *value {
                                *value = affix.len();
                                pb.println(&format!("New affix length {}", *value));
                            }
                        }
                        None => {
                            stats.affix_max = Some(affix.len());
                        }
                    }
                },
            }

            pb.inc(1);
            pb.inc_length(children.len().try_into().unwrap());

            for child in children {
                tx.send(child).unwrap();

            }


        });

    }

    // println!("Occupied nodes max {:?}", node_occupied.iter().max());
    // println!("Occupied nodes min {:?}", node_occupied.iter().min());
    // println!("Total nodes {}", node_occupied.len());
    // let state = EngineState::new(global_state, EngineConfig::default());

    // let keys_to_purge: Vec<Key> = (0..latest_era).map(EraId::new).map(Key::EraInfo).collect();

    // println!("Purging {} keys with batch size of {}... Total batches {}", keys_to_purge.len(), batch_size, keys_to_purge.len() / batch_size);

    // let start = Instant::now();

    // for (i, keys) in keys_to_purge.chunks(batch_size).enumerate() {
    //     let purge_config = PurgeConfig::new(state_root_hash, keys.to_owned());

    //     let commit_purge = Instant::now();
    //     let result = state
    //         .commit_purge(CorrelationId::new(), purge_config)
    //         .expect("should purge");
    //     let purge_elapsed = commit_purge.elapsed();

    //     match result {
    //         PurgeResult::Success { post_state_hash } => {
    //             // if post_state_hash == state_root_hash {
    //             //     println!("(!) No purge was performed.");
    //             // } else {
    //             // }

    //             // println!("State root hash after purge: {}", post_state_hash,);
    //             println!("Purge chunk {} elapsed {:?}", i, purge_elapsed);
    //             state_root_hash = post_state_hash;
    //         }
    //         other => panic!("Expected success but received {:?}", other),
    //     }
    // }

    // let elapsed = start.elapsed();

    // println!("Purge finished in {:?}", elapsed);

    true
}
